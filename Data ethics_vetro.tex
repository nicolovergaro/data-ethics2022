\section{Data ethics (Vetrò)}
\subsection{Premises}
Some people say that data is the new oil. While this can be considered true, expectations were inflated. We live in a time where data is in fact exponentially growing and more companies decide to take data driven decisions, especially banking and insurance companies. This is bringing us to the next industrial revolution.

Some definitions:
\begin{itemize}
    \item Cyber-physical sytems: based on networked embedded software systems, which connect computational entities with physical entities to achieve an overall purpose.
    \item Smart agent: basic element of an AI system or smart system. It perceives and acts in an environment. The agent function specifies the action taken by the agent in response to any percept sequence. How well is the behaviour is defined by the performance measure, which evaluates the behaviour of the agent to decide if the agent is rational and which actions maximize the performance value. An agent usually has sensors through which it receives data that is given as input in an algorithm. The algorithm decides the actions to perform through actuators. In socio technical systems actuators make actions on people. (telepass system). 
\end{itemize}
We can say that technologies are tied to society, but our perspective is on inequalities, in particular we want to analyze if technology preserves, reduces or enhances existing inequalities in society.

Why data ethics?
We won't focus only on law but also on ethics. The next question we will ask is: what is ethics? It's a branch of philosophy, which is used to understand what is good and what is bad, not what's right and wrong. In our context it deals with decisions taken by machines and to the principles that we refer to in order to understand if those decisions are acceptable from individual and society point of view.

In our course we focus on certain aspects of ethics. In a study of 2016 the philosopher Floridi and other colleagues mapped the debate on ethics of algorithms. Most of the issues were about:
\begin{itemize}
    \item inconclusive evidence, inscrutable evidence, misguided evidence: the data we are using to train algorithms are not good enough (inconclusive), we can't look at the data (inscrutable) or data can be wrong (misguided). 
    \item unfair outcomes, transformative effects: data is used to obtain an outcome which will trigger and motivate actions that may not be ethically natural.
    \item traceability: who is responsible for decisions taken? designer, programmer, manager?
\end{itemize}
We will mostly focus on unfair outcomes and transformative effects.

\subsection{Introduction to data ethics}
Automated decision making systems (ADM), in our socio-technical perspective are not only algorithms that define the rules, but also the model, the data that is used, and the context they are embedded in. All these elements must be taken into account. Algorithmic systems are not ADM, but they are a part of it.

ADM systems learn from data and to have a good behaviour, data should be:
\begin{itemize}
    \item sufficiently large;
    \item sufficiently heterogeneous;
    \item labelled with the "right" answers (but we don't know who labels data);
\end{itemize}
ADMs, by learning from examples, perform an inductive process. There are three types of inferences:
\begin{itemize}
    \item Deductive \footnote{Christmas is always Dec. 25th; today is Dec. 25th, therefore it’s Christmas. The prefix de- means "from," and deduction derives \textit{from} generally accepted statements or facts}: you make rules (general or universal premises) and make inferences based on rules. Can't go against rules. 
    \item Abductive \footnote{A detective's identification of a criminal by piecing together evidence at a crime scene. Abduction will lead to the best explanation. The prefix ab- means "away," and you take \textit{away} the best explanation in abduction}: deals with our reasoning and how we put elements together. I can observe reality, make reasoning and then make inferences. 
    \item Inductive \footnote{At lunch you observe 4 of your 6 coworkers ordering the same sandwich. You induce that the sandwich is probably good. Your reasoning is based on an observation as opposed to universal premises. The prefix in- means "to" or "toward," and induction leads you \textit{to} a generalization}: machine learning models. Start from data and find patterns, then make inferences. 
\end{itemize}
Abductive and inductive share properties: conclusion goes beyond premises and we can can still make conclusions with a subset of the premises.

When modelling there are some potential problems in the use of historical series:
\begin{itemize}
    \item reality is a super set of what is measurable (low hanging fruit).
    \item some aspects of society are measurable only indirectly.
    \item societies have historical and structural inequalities reflected by the data.
    \item spurious correlation in the data can often be found.
\end{itemize}
Example: demographic disparities. e-commerce company analyzes historical data of online purchases and demographic data to decide where to enable fast delivery services. After the analysis it turned out that fast delivery were offered where people were prevalently white. The source code of the algorithm and the dataset were not publicly available. The effect was a discrimination based on skin color.\\
Example: COMPAS case. system to decide whether to keep a person in prison or decide other ways to be punished after a problem with the justice. COMPAS outputs low, medium or high risk of recidivism. The impact was a systematic discrimination towards a specific ethnic group. The source code and the data are not publicly available because it's a private company, but it's used by a public body.

There are private IT companies that have a higher market capitalization than the GDP of states like Canada, Italy, etc. This could create a problem because they have a lot of power but they are private. They don't have the same accountability measures as public entities, this can be a problem for decisions that impact the society. Another fact is that in the largest companies Europe is not present, can't be competitive with USA and China. 
An example of their power is the contact tracing mechanisms in which Google and Apple decided how to perform it and all the world had to adapt.

There are also many examples of biases in the data, such as:
\begin{itemize}
    \item A research of the word "CEO" leads to mostly images of men, while the word "nurse" leads to women.
    \item Toxicity score of the same sentence is worse if instead of "French" there is "Black man".
\end{itemize}
We investigate the relationship between technology and society and whether the inequalities in society are amplified by technology. As reference point we can take art.21 of the EU charter of fundamental rights, in which is stated that any discrimination of any nature is prohibited.

\subsection{Demographic disparities in the loop}
The loop is the machine learning loop, where the data of the individual makes the state of the world, that is the description of a phenomena. Through measurements it becomes data, which is fed to an algorithm that take decisions on individuals; this closes the loop. If individuals change their behaviour after they received an action from the model this might cause changes in the model. 

\subsubsection{State of the world}
Disproportions and inequalities are common in our societies, for example certain groups may be concentrated in certain areas of the city/region; certain groups have a different way to use technology that changes by age, ethnic groups, etc. 

The matrix of domination/oppression is a sociological paradigm that explains issues of oppression with with race, class, gender, sexual orientation, religion, or age. These can be seen as the main drivers for inequalities. The dominant group is a Men, anglo/american, rich, educated, able, english, young and heterosexual. We can take this as a good reference and examples seem to support this. For example, the airport scan. Some people don't identify with their biological sex and this brought dysaffordances, which is what technology prevents you from doing. In the example a dysaffordance is the selection of sex from a static form.

We now move from the state of the world to the measurement process that gives us data. In the example regarding over and under representation in universities, the category "multi racial" was introduced in 2008, so what happened before? We can't trust the part of data before 2008. 

\subsubsection{Measurement process} 
We bind a number to an entity. Usually we say that data is objective, but that's not entirely true, since in the measurement process many subjective choices have to be made.

Example: rewarding system for developers. The company decided to use the number of lines written,  the defects fixed and the number of hours. This brought some problems such as:
\begin{itemize}
    \item some languages are more verbose than others;
    \item time spent of related activities is not taken into consideration;
    \item reflexivity problem: The more any quantitative indicator is used for decision-making about people, the more it will be subject to the pressures of corruption. For example a researcher will push its research as soon as possible because of an indicator to meet constraints.
    \item Goodhart's law: When a measure becomes a target, it ceases to be a good measure
\end{itemize}

Follow up example: the company decides to reward a group instead of an individual. In this case they use the average productivity and two managers rewards two different groups, because of the different way the calculations are done (defects solved per hour versus hours to solve a defect). This is caused by Simpson's paradox, for which if f is not linear, $E[f(x)] \neq f(E[x])$.

\subsubsection{Learning}
Now we move from data to the model, analyzing the learning process, which can be dangerous because if data is biased it will enter the model. Examples are Google translate which translate from languages that have a single pronoun for he/she/it that has a bias.

\subsubsection{Action}
To analyze the last part of the cycle we introduce the field of predictive policy, with which we try to predict where and when the next crime will happen to try to allocate police forces better. They tried to model the behaviour of people by analyzing historical data about arrests, this is an epistemic approach\footnote{if it happened in the past it will happen in the future}. The problem is that people are not that predictable and also change over time. Another problem is that they used general trends to predict individual behaviours. This led police to "expect" a crime and to act consequently, by for example performing arrests where the software predicted crimes would happen. Using arrest data (biased) also brought more black people to be stopped, while medical data show that the distribution of race is mostly uniform. 

Finally, the \textit{feedback loop} problem. If you keep feeding the algorithm using data from the algorithm itself, you keep arresting the same people. Same effect in social media, which creates an "echo chamber" by reinforcing the same patterns.

To conclude, optimization algorithms define thresholds. By changing the threshold you can have a different impact on different people. A famous example of using general trends and applying it to an individual is the Raffray-Calment story

\subsection{Case study: The COMPAS case}
The COMPAS\footnote{Correctional Offender Management Profiling for Alternative Sanctions} system is used in some U.S. court systems to estimate the probability of recidivism of a convicted offender.
COMPAS brought systematic discrimination towards black people. In fact there are example of black people being classified as "high risk" who committed minor crimes, while white people being classified  as "low risk" and committed more serious crimes. False positive rate (high risk but didn't re-offend) is higher for African-Americans while false negative rate (low risk but re-offended) is higher for white people.

\subsection{Case study: the Facebook advertising platform}
We will look at a scientific study that shows empirically the discrimination problem in advertisement in Facebook. Then we will see the actual case of HUD vs. Facebook. The case doesn't use data from the study. Let's start from the findings of the paper.
\subsubsection{Research}
How did it work at time of the research? There are 5 elements in an ad:
\begin{enumerate}
    \item Headline
    \item Image/video
    \item Domain 
    \item Title
    \item Description
\end{enumerate}
3,4 and 5 are automatically pulled from the website you will visit if you click on the ad. There are 2 separated phases:
\begin{enumerate}
    \item The advertiser creates the ad and decides: which content to advertise, which users should see the ad (targeting) and finally the bidding strategy.
    \item Ad is delivered to people. In this phase the advertiser has no control.
\end{enumerate} 
Now let's see the effects analyzed by the paper. We want to know if one of these has an effect on the type of audience, regardless of the audience selected in the previous phases.
\begin{enumerate}
    \item Budget
    \item Ad creative
    \item Ad image
    \item Ad image classification: performed by the platform.
    \item Test on real-world ads: in the previous steps the ads were fake to test those aspects. This was done because researchers wanted to test if the platform inserted other constraints on the definition of the audience. The defence of Facebook was that the platform was neutral.
\end{enumerate}
The territorial context are four specific regions in North Carolina for demographics reasons, the first two target white people, while the second two target black people.

The first experiment was to check if the budget had an effect on the percentage of males who saw the ad. The higher the budget the lower the percentage of men.

The second experiment was about Ad creative. Different combinations of Ad component and topics (body building, cosmetics) were tried. They checked the percentage of male audience. The amount of budget was fixed. Researchers found that when an image is inserted there is a significant discrepancy in fraction of men for cosmetics and bodybuilding. So the platform intervened.

The third experiment focused on the image. The topics were the same from before but the image was tested in two scenarios, with the correct image or with a swapped image (e.g cosmetic image when the topic was bodybuilding). In this case they found that the platform classified the image and targeted different people regardless of the topic.

Next they tried to vary the level of stereotype and three levels of image: Visible, blank and invisible (filter that makes it invisible to human eye). What they found is that even when the image is invisible the platform still intervened by classifying the image and repeating the same behaviour as before.

The final test is on real word ads and they designed three types of ads:
\begin{itemize}
    \item music type: they checked the percentage of white people. It was found that there was a relationship between the genre and the percentage of white men in the audience targeted.
    \item employment offers: they tried different job titles and different images. They checked on the percentage of men and white users. We can notice large differences for secretary and in particular when there are black people.
    \item housing offers: they tried different type of property, cost and black/white/no family as image. They checked for white users and men users.
\end{itemize}
\subsubsection{HUD v Facebook}
Until 2020 you could select the ethnic group which was declared or inferred. Now there is the African-American culture because Facebook was charged by the Department of housing and urban. In particular this document (which didn't use data from the previous studies) focuses on the housing market. The \textbf{premises} are that you can't discriminate by race, sexual orientation, sex, nation, etc regarding housing for renting, selling, or accessing the market to certain people. The \textbf{factual allegations} are that Facebook was paid by customers to show the ads to difference kind of audiences which could be customized in a discriminating way, for example by ethnic group, zip code, sex etc. Facebook also created its "actual audience" by using an algorithm. The advertiser could not "broadcast" the message but had to select an audience. Also Facebook charged differently to show the same ad to a different group of people. All this behaviour is because Facebook shows the ad to people that are likely to interact with the ad. The \textbf{legal allegations} are that Facebook discriminated by making dwellings unavailable, refused to publish advertisements, denied information about dwellings, charged differently for advertisements; all based on race, color, sex, etc. This trial is still ongoing and if Facebook loses the case, all employees must attend a training on the Fair Housing's Act about advertisements and obviously pay to all damaged people.
\subsubsection{Algorithmic Accountability Act of 2019}
\textit{This is not a law}. In this act it was defined an \textbf{automated decision system} and its \textbf{impact assessment} based on the description of the system, design, training data. Also it should contain the benefits and costs of the system in terms of data minimization, storage time or what information are available to the public. It should also contain an assessment of the risks to privacy or security of personal information and what are the measures taken to mitigate the risks. It was also given the definition of a \textbf{covered entity}, which are the entities that this act refers to, that are entities having more than \$50mil annual gross receipts or that had information about 1mil users/1 mil devices, that are controlled by corporations falling in the two previous categories, or that are data brokers. If a company is a covered entity then it must provide an impact assessment prior to implementation.
\subsubsection{Local law of the city of New York}
\textit{This law passed and is effective}. It is a law about the use of automated employment decision tools. It was defined the \textbf{bias audit} as an impartial evaluation from an independent auditor, which is mandatory. Should assess the impact of the tool on people. This law affirms that the city should not use any automated employment tool unless it was certified with an audit less than a year prior, or also if the results of the audit are public on a website.
\subsubsection{Banning Surveillance Advertising Act of 2022}
\textit{This is an ongoing act.} To prohibit targeted advertising. You can't use personal data to make targeted ads.
\subsection{Formalization of Algorithmic Fairness}
FOR THE EXAM, STUDY TABLES AND HOW TO COMPUTE FAIRNESS VALUES!!!
When we talk about \textbf{fairness criteria} we deal with the impact of an algorithm, especially we try to understand whether the impact is a \textit{disparity impact}, which means that outcomes are different for a specific social group identified by a protected attribute\footnote{also sensitive attribute: attributes that shouldn't be linkable to an individual e.g sex, race, sexual orientation, etc.}.
Indicating with:
\begin{itemize}
    \item R : classifier 
    \item Y : target variable
    \item A : sensitive attribute
\end{itemize}
The fairness measures we will see are:
\begin{itemize}
    \item Independence: classification should be independent from the sensitive attribute:
    $$
        R \perp A: P[R=1|A=a] = P[R=1|A=b]
    $$
    Can also introduce a tolerance, given that obtaining two exactly equal probabilities is difficult. In the COMPAS case, independence given African American is 0.58, while it's 0.33 with Caucasian. We can achieve independence:
    \begin{itemize}
        \item Pre processing: removing proxy variables or balancing the dataset.
        \item At training time: tweaking details of optimization process.
        \item Post processing: adjust the classifier and try to make the classifier uncorrelated with the sensitive variable.
    \end{itemize}
    Independence can be applied at any stage but it ignores the correlation between Y and A. Also can bring a really low accuracy for one of the groups and to exchange false positive for false negatives.
    \item Separation: requires independence from the protected attribute but conditional on the target variable:
    $$
        R \perp A | Y: P[R=1|Y=1, A=a] = P[R=1|Y=1, A=b]
    $$
    And the same for $Y=0$, so the false positive and false negative rate is the same for all the values of the protected attribute. If the classifier is not binary it should hold for all levels. We can introduce a tolerance also in this case. 
    In COMPAS we had for false positives 0.22 for Caucasian, 0.42 for African American (advantage for African American) and for false negative 0.5 Caucasian, 0.28 African American (advantage for Caucasian). It can be obtained at training by ad-hoc optimization, or in post processing at the intersection of the ROC curve of each group. Separation takes into account the target variable but is much more difficult to apply and doesn't take into considerations false negatives.
    \item Sufficiency: the target variable should be independent from the protected attribute given the classification:
    $$
        Y \perp A | R: P[Y=1|R=r, A=a] = P[Y=1|R=r, A=b]
    $$
    This is equivalent to \textit{calibration}. 
    In COMPAS positive predictive value Caucasian=0.59, AM: 0.65, while negative predictive value Caucasian=0.29, AM: 0.35. These scores are pretty similar.
\end{itemize}
Fairness criteria, taken in pairs, are mutually exclusive. Case by case we should take into account different fairness criteria. In COMPAS, if we look at data based on age we can see that percentages among recidivists, low and high risk are different, so how should we act? It's difficult because going to prison might be difficult for an older person, while for a young person the young age is important also in the change of behaviour. Finally, a study compared COMPAS predictions to human predictions with no or little expertise on criminal justice. The results were very similar but in this case it's better to use a machine because it saves money.
\subsection{Algorithmic Fairness: some reflections}
\subsubsection{Reflection \#1: Fairness and discrimination}
If we take fairness as absence of discrimination we should apply the same treatment to all people based on their characteristics. Characteristics:
\begin{itemize}
    \item Intrinsic or acquired: based on your own characteristics that you have by birth (ethnic group, place of birth) vs acquired: job, city where you live,..
    \item absolute vs context-dependent.
    \item permanent vs temporary.
\end{itemize}
There are different types of discrimination, which are:
\begin{itemize}
    \item Direct: the personal traits will result in a non favorable outcome. Similar to disparate treatment. Commonly seen in computer science as protected attributes. These are sex, race, color, religion,.. Protected attributes are specified in EU Primary/Secondary laws and also in the US with domains that protect special social groups (Equal Credit Opportunity Act, Fair Housing Act). Most of these are results of Martin LK's work.
    \item Indirect: discrimination based on proxy attributes, for example zip code in the Amazon case. Similar to disparate impact.
    \item Systemic: discrimination is embedded in the social norms of the culture, for example employers that prefer candidates that share similar experiences or that come from the same region. 
    \item Statistical: decisions are based upon average group statistics to judge an individual belonging to that group (similar to Raffray-Calment story)
    \item Explainable: discrimination happen and you can explain why, for example higher annual income of men with respect to women. This can be explained as difference in worked hours, maternity. The fact that the discrimination is explainable can still make it illegal.
    \item Not explainable: discrimination happen but you can't explain why. facebook case.
\end{itemize}
To understand a discrimination we should understand:
\begin{itemize}
    \item Nature: what is the harm and who does it afect?
    \item Severity: for each affected person, how severe is the harm?
    \item Significance: how many people are disadvantaged and what is the level of disadvantage? With algorithmic fairness we can make inferences on significance.
\end{itemize}
\subsubsection{Reflection \#2: No universal mathematical formalization of fairness exists}
This is also known as impossibility of fairness. In this course we dealt with \textbf{group fairness}, based on social groups. We also have \textbf{individual fairness}, which means that similar individuals should get similar predictions. Finally, there is also \textbf{subgroup fairness}. 

We will see individual fairness and alternative of group fairness.
In particular, for individual fairness:
\begin{itemize}
    \item Fairness through awareness: any pair of similar individual should receive a similar outcome. Depends on the task, the similarity measure and the outcome threshold.
    \item Fairness through unawareness: you don't use protected attributes but we know that discrimination is possible for proxy attributes.
    \item Counterfactual fairness: you compare results between the same instance when you change the value of the protected attribute. You measure how many instances have a different classification. This can be done with alternating functions. Counterfactual because you have to imagine a world where the same individual belongs to a different group.
\end{itemize}
Regarding group fairness, we already saw Independence, Separation and Sufficiency. Examples of alternative measures are:
\begin{itemize}
    \item Treatment equality: ratio of false negative and false positives and check that they are the same for all the protected group categories.
    \item Fairness in relational domains: not only protected attributes but also social or organizational aspects, for example sex and place of birth. 
    \item Conditional statistical parity: you don't take protected attributes but a set of factors.
\end{itemize}
\subsubsection{Reflection \#3: Mathematical notions of fairness apply only to observational data}
\subsubsection{Reflection \#4: Fairness as Justice: equity, equality, need}
Now we use the concept of fairness in terms of justice. Justice is an immense field, we refer to justice as adherence to law or as equality. If we focus on the equality notion, we have two categories:
\begin{itemize}
    \item Context of giving and receiving: typically characterized by bilateral relationship. If I provoke a damage I should pay. Notions of equivalence between things. Regards exchanges. \item Field of assigning advantages,disadvantages: multilateral and unidirectional, where you have a body that assigns benefits or obligations. Equivalence between persons, not things and values. Regards relationships of coexistence. Attributive justice (how much should I expect from a given group)
\end{itemize}
One of the founding principle is the concept of equality of opportunities: everyone should have the same starting conditions. Equals are treated equally. The situation is not linear, there are three paradigms of distributive justice:
\begin{itemize}
    \item Equality: outcomes are allocated equally.
    \item Equity: outcomes are allocated according to contributions.
    \item Need: outcomes are allocated according to needs.
\end{itemize}
They should be chosen by the situation. 
\subsubsection{Reflection \#5: From values to decision making}
What are the principles and the values that you follow when you choose one paradigm or the other? Usually values shape the rules that are then implemented. For example assigning or denying loans could be decided by different underlying values, for example safeguard of lender's capital, effect on environment, etc. Then they determine what are the rules, for example to safeguard the lender's capital, you could take a debt/income less than x\%, or value or properties already owned by him or relatives. Finally the rules are implemented.

FOR THE EXAM, explain why a discrimination occurs or what are the risk factors. could you imagine a different goal for the algorithm? for example for compas case, it could be used for policy makers, where arrests/criminality is a problem.

\subsection{Fairness Qualitative Assessment}
We have \textbf{algorithm audit}:
\begin{itemize}
    \item bias audit: overall approach to assess bias in algorithm and dataset. It is an ex-ante approach.
    \item regulatory approach: ex-post approach to check compliance with regulations and norms of the algorithms developed.
\end{itemize}
On the other hand we have \textbf{algorithmic impact assessment}, to understand if there is an impact of the algorithm. 
\begin{itemize}
    \item algorithmic risk assessment, which is done ex-ante, in this course we focus on discrimination but there are broader impacts, like environmental impact, for example to train a model there is an impact on the environment.
    \item algorithmic impact evaluation, which is ex-post. The tools we will see are for risk assessment but they can be used also for impact evaluation, depending if we use before or after production.
\end{itemize}
Regarding \textbf{fairness assessment}, it is a methodology to identify issues of fairness connected with power asymmetry. It can be performed before or after deployment by developers, policymakers, students. The more you have input from impacted stakeholders the better it is, because you can be much more precise instead of making hypothesis. 
The assessment is based on four questions, that are:
\begin{enumerate}
    \item What social conflict arises from the design/operation of a system? 
    \item What constituencies are adversely affected by this conflict? How?
    \item What social interests are at stake in this case?
    \item How could the conflict be resolved fairly?
\end{enumerate}
The example is the saferroute system, that uses police historical data to improve the safety of the city. You have a navigator that suggests you the safest route and avoids going through areas with highest percentage of crimes. The same data used in the predictive policing case can be used to predict the best route. We will use the Taylor Russel diagram (confusion matrix) to check if safe areas were incorrectly classified as dangerous and vice versa.
\subsubsection{What social conflict arises from the design/operation of a system?}
Some criticalities we can think of are:
\begin{itemize}
    \item What are the thresholds to determine if an area is safe or dangerous or what is the denominator of the probability calculations.
    \item what is the construct we are measuring, that is what kind of criminality are we measuring
    \item finally, it inherits the limitations of predictive policing, such as the bias involved in the measurements is fed in the algorithm.
\end{itemize}
\subsubsection{What constituencies are adversely affected by this conflict? How?}
First of all we define who are stakeholders, that can be classified in:
\begin{itemize}
    \item \textbf{direct} stakeholders, which are the users, that means a person that uses the technology.
    \item \textbf{indirect} stakeholder, which are other persons that don't interact with the application but might be impacted by it.
\end{itemize}
In saferroute the \textit{direct stakeholders} can be all those that are new to the city or women who can be physically vulnerable. An intersection could be female students. 

The \textit{indirect stakeholders} are people or businesses living or operating in the dangerous areas or police that are in the involved areas. Indirect stakeholders can also be people in safe areas because criminals could move to safe areas due to less control. 
Given the stakeholders, how could they be adversely affected? 
\begin{itemize}
    \item \textbf{False positives} can cause a reputation damage to the misclassified area, potentially labeling as "ghetto", thus making residents' properties lose value or making businesses lose clients.
    \item \textbf{False negatives} affect users that are routed through safe areas but are assaulted.
\end{itemize}
This tools allows us to understand the negative effects of the usage of the application. 
\subsubsection{What social interests are at stake in this case?}
Given that we identified the negative impacts, now we ask what are the social interests at stake. We have to think in terms of values, which are principles or standards of a person or society. Personal/societal judgement of what is valuable in life.

In this course we focus on human welfare values (education, health, income), since automation and automated decision making is proceeding very fast in these fields. A concrete example of values is for example, the MIT moral machine where you are asked to decide who to save in a autonomous driving scenario.
Some examples of values are:
\begin{itemize}
    \item Autonomy: people's ability to decide, plan and act. This is for example in contrast with nudging that we see in apps nowadays.
    \item Trust: expectation that exists between people.
    \item Usability: making all people successful users of technology, for example technology that can be used by people with disabilities.
    \item Informed consent: achieving an effective agreement on the usage of personal data.
\end{itemize}
Some values come from IT world, other come from philosophy. Values in software can be: 
\begin{itemize}
    \item \textbf{Explicitly supported} in the form of design constraints, requirements, business principles.
    \item \textbf{Designer values}, personal or professional values that a designer brings into the design of a tool, which are not necessarily aligned with the company (e.g. Google, an employee stopped a project involved with the military
    \item \textbf{Stakeholder values}, for example the values we saw in saferroute case. Usually these values are inferred via business surveys or directly asked through participatory design principles: asking representatives of stakeholders to give a feedback to the application. In particular this approach is called  human centered AI, and it's useful because the more diverse the feedback is the better it is for the application.
\end{itemize}
To understand what interests are at stake in saferroute we check the confusion matrix and focus on the errors. When we have a false negative we are going against the value of personal safety of the user. With false positives, the more you are going against the values of economic and reputation security, the less happy residents and businesses will be to live in a not safe area. 
\subsubsection{How could the conflict be resolved fairly?}
For example we could:
\begin{itemize}
    \item solicit public feedback from the design phase
    \item also reason with the population what could be the best way to signal danger
    \item enable software error reports
\end{itemize}
You might decide to give priority to one value or make a compromise, for example if you value more safety you should tune the predictor to be more conservative as possible.

FOR THE EXAM, theory about the methodology (how is the fairness assessment organized?) or a piece of the case could have a secondary question about the methodology (what are other stakeholders not cited in the text?). not apply the methodology.

\subsection{Measuring balance in datasets}
Imbalance is a disproportion of occurrences of a given class in a dataset. For example in the COMPAS case there was a disproportion in the sensitive attribute "race". A disproportion in a sensitive attribute can cause discrimination. Balance measures will be applied to the training set, to measure if there is a risk of discrimination; Fairness measures, on the other hand, will be considered in the testing phase.

The measures we will look at are measures on categorical data, because most of the data on sensitive attributes is categorical or is made categorical, for example age is continuous but it's often divided in intervals. The measures are:
\begin{itemize}
    \item Gini index: to measure heterogeneity. 
    \item Simpson index: to measure diversity.
    \item Imbalance ratio: the only measure that comes from machine learning. Used to measure imbalance
\end{itemize}
\subsubsection{Heterogeneity}
How many different types are represented in a dataset. It comes from different disciplines. The higher the heterogeneity (close to 1) the more equal are the frequencies, so the more balanced is the dataset. With lower values, frequencies are concentrated in a few classes. The most important measure is the \textbf{Gini-Simpson index}, that should not be confused with the Gini index in Economics. Computed as:
$$
    G = 1 - \sum_i^m f_i^2
$$
Where we have all the relative frequencies $\left(f_i = \frac{n_i}{\sum_i^m n_i}\right)$ squared and summed. It goes from the minimum heterogeneity where G=0 to the maximum $G=\frac{m-1}{m}$. We can then normalize the index obtaining:
$$
G_n = \frac{m}{m-1}\left(1 - \sum_i^m f_i^2 \right)
$$
We normalize because we want this measure to be comparable to others. In the calculations if some class contains 0 elements we can decide to count or not to count them in the number of classes, but we must keep this decision consistent also for other measures.
The Gini index is \textit{superlinear}.
\subsubsection{Diversity}
We take from other domains, provides information about how diverse a community is. We see the \textbf{Simpson diversity index}:
$$
    D = \frac{1}{\sum_i^m f_i^2}
$$
It represents the probability of belonging to different classes. Higher values of Simpson index indicate higher diversity, so balance is higher.
The minimum value, which means the lowest diversity, so there is only one class, is 1, the maximum value is m. In this way we can normalize the index, which becomes:
$$
    D_n = \frac{1}{m-1} \left(\frac{1}{\sum_i^m f_i^2} -1 \right)
$$
The Simpson index is \textit{almost linear}.
\subsubsection{Imbalance}
The \textbf{Imbalance Ratio} (IR) is widely used in machine learning. It is the ratio between the highest and the lowest frequency:
$$
    I = \frac{\max f_i}{\min f_i}
$$
If you take into account 0 as min frequency it goes to infinity, so in general we don't count the classes that don't have occurrences. 

The minimum and the maximum are both 1, so we take the inverse to make it coherent with other indexes. Also because we want to measure balance, not imbalance.
$$
    I = \frac{\min f_i}{\max f_i}
$$
The Imbalance Ratio index is \textit{sub linear}, so it goes to 0 very quickly as it's very reactive to imbalanced situations. For example if we have m-1 classes with similar values, and 1 class that has a really low value, the index will immediately show it.
\subsection{Source of bias: an overview}
We are not speaking about statistical bias, when the expected value of a an estimator is the estimator itself. In this case we talk about bias in software systems: computer systems that \textbf{systematically} and \textbf{unfairly} discriminate against certain individuals or groups of individuals in favor of others by denying an opportunity for a good or assigning an undesirable outcome to an individual or groups of individuals that are unreasonable or inappropriate.
\subsubsection{Taxonomy of bias in 1996}
\begin{itemize}
    \item Preexisting bias
    \begin{itemize}
        \item Individual
        \item Society 
    \end{itemize}
    \item Bias in technology
    \begin{itemize}
        \item Computer tools
        \item De-contextualized algorithms
        \item Random number generation
        \item Formalization of human constructs 
    \end{itemize}
\end{itemize}
\subsubsection{Sources of bias in a typical ML process}
In a typical ML process there are many steps in which we can inject bias. We can see how the process evolves ( biases will be highlighted in \textit{italics} ).
The relation between X and Y (\textit{Historical}) is what we want to learn. Through measurements steps we can have a sample (\textit{Representation}). We then select features and labels (\textit{Measurements}) and obtain two estimates of X and Y. The relationship between these two estimates is the learnt function (\textit{Aggregate}). Finally we have a measure of success and we evaluate what we learnt to other data (\textit{Evaluation}).
\begin{itemize}
    \item Historical bias, represents inequalities and disproportions in the real world.
    \item Representation bias, introduced through sampling/measurements. For example when we don't measure all the population, when there are changes in the population not detected or the Simpson Paradox. Practical examples are slangs in social networks or training an image classifier not considering all the domain.
    \item Measurement bias, which is introduced when performing selections. For example when granularity and quality of data vary across groups. For example the lack of historical data for people that moved to a new country, or feature selection by subjective choices. Another example is the selection of proxy variables. A practical example is having richer traffic data on urban areas than rural areas.
    \item Aggregation bias, that arises in the relationship between the features and the target attribute. For example when using clinical tools that might need a different parametrization for specific ethnic groups or reviews on web platforms that might be influenced by past reviews (people tend to confirm past reviews).
    \item Evaluation bias, when training data is very different from evaluation data or when evaluation data don't represent the true distribution of the population. For example a facial recognition system mostly trained on white faces and used/evaluated on other skin colors.
\end{itemize}
\subsection{ACM code of ethics and professional conduct}
ACM stands for Association for Computing Machinery. It is based on four parts:
\begin{enumerate}
    \item Describes general ethical principles of a computing professional;
    \item Professional responsibilities. More practical than ethical principles
    \item Specific responsibilities for those who have leadership, such as managers.
    \item How to be compliant and what happens if you don't comply.
\end{enumerate}
This is a drastic change in focus, in fact the first line says that "Computing professionals' actions change the world.[...] They should reflect upon the impact of their work, consistently supporting the public good.". 

Addressed to everyone involved in computing, firstly practitioners, that are people who work in companies.

The code is not an algorithm to solve ethical problems, it just gives guidelines and principles for ethical decision making.
\subsubsection{General ethical principles}
\begin{itemize}
    \item 1.1 Contribute to society and to human well-being, acknowledging that all people are stakeholders in computing. 
    
    You have an obligation to consider the impact of your work on society. It includes promoting fundamental human rights and each individual autonomy. 
    
    You have an obligation to minimize negative consequences of the tools that you are developing, such as threats to health, safety, etc.
    When the interests of multiple groups conflict, priority should be given to the less advantaged. 
    
    You should consider if your work respects diversity, will be used in responsible ways and is accessible.
    
    You must pay attention to work in a safe natural environment, by promoting environmental sustainability.
    \item 1.4 Be fair and take action to not discriminate. 
    
    You should foster fair participation of all people; discrimination based on "sensitive attributes" is a violation of the code.
    
    Consider the impact of new technology on inequalities, which may be enhanced. You should take action to mitigate this problem.
    \item 1.6 Respect privacy. 
    
    You should take precautions to prevent re-identification of anonymized data, make sure that data is accurate, understand the provenence of data.
    
    Only the minimum amount of data necessary should be collected.
\end{itemize}
\subsubsection{Professional responsibilities}
\begin{itemize}
    \item 2.1 Achieve the highest possible qualities in your works.
    \item 2.2 Mantain high standards of professional competence, conduct and ethical practice. 
    
    Professional competence starts with technical knowledge and awareness of the social context in which your work will be deployed. Professional competence also requires skills in communication, reflective analysis, recognizing and navigating ethical challenges.
    \item 2.5 Give comprehensive and thorough evaluations of computer systems and their impact, including analysis of possible risks. 
    
    Evaluations should not only be given to employers or employees, but also to the public. \textbf{Extraordinary care} should be taken to identify and mitigate potential risks in machine learning systems and if risks are high you should consider whether to deploy the system or not. This approach is a change in focus, in fact we can see that it's not innovation first, but people first. Risk assessments should be continuous as systems evolves in use.
\end{itemize}
\subsubsection{Professional leadership principles}
Recognized for both leaders that have a formal designation, but also people that are informally leaders from influence over others. 
\begin{itemize}
    \item 3.1 Ensure that the public good is the central concern during all professional computing work.
    
    You should keep this focus regardless of the methodologies and the techniques used.
    \item 3.7 Recognize and take special care of systems that become integrated in the infrastructure of society.
    
    This article is included because computing systems are becoming more and more prevalent. As the level of adoption changes, the ethical responsibilities have to change as well. When appropriate standards of care do not exists, you have a duty to ensure they are developed.
\end{itemize}
\subsubsection{Compliance with the code}
\begin{itemize}
    \item 4.1: Uphold, promote and respect the principles of the code.
    
    The future of computing depends on both technical and ethical excellence.
\end{itemize}
